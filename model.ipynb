{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8f49292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from hyperopt import hp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.csgraph import laplacian\n",
    "from scipy.sparse.linalg import svds\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "from datasets import Dataset\n",
    "import layers\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2432a86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, session, dataset, **config):\n",
    "        # tensorflow Session object\n",
    "        self.session = session\n",
    "        # datasets.Dataset object\n",
    "        self.dataset = dataset\n",
    "        # dict of hyperparameters, etc.\n",
    "        self.config = config\n",
    "\n",
    "        # inputs\n",
    "        self.id = tf.placeholder(tf.int64, [None])\n",
    "        #placeholder()函数是在神经网络构建graph的时候在模型中的占位，此时并没有把要输入的数据传入模型，它只会分配必要的内存\n",
    "        self.user_id = tf.placeholder(tf.int64, [None])\n",
    "        self.item_id= tf.placeholder(tf.int64, [None])\n",
    "        # labels\n",
    "        self.r_true = tf.placeholder(tf.float32, [None])\n",
    "        \n",
    "        # define model parameters\n",
    "        self.weights, self.biases = self._params()\n",
    "        # define rating computation and scale to dataset range Xij公式\n",
    "        self.r_pred = self.dataset.min + self.dataset.range*tf.sigmoid(self._r_pred())\n",
    "        # define loss\n",
    "        self.loss = self._loss()\n",
    "        # define rmse metric for update monitoring #平方开根号\n",
    "        self.rmse = tf.reduce_mean((self.r_pred - self.r_true)**2)**0.5\n",
    "        \n",
    "        # Adam optimization with default learning rate\n",
    "        self.opt = tf.train.AdamOptimizer().minimize(self.loss)\n",
    "\n",
    "    def _params(self):\n",
    "        # see example models below\n",
    "        weights, biases = {}, {}\n",
    "        return weights, biases\n",
    "\n",
    "    def _r_pred(self):\n",
    "        # see example models below\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _loss(self):\n",
    "        self.mse = tf.reduce_mean((self.r_pred - self.r_true)**2)\n",
    "        self.reg = self._reg()\n",
    "        # alpha hyperparameter controls regularization, range 0 to 1.\n",
    "        self.alpha = float(self.config.get('alpha', 0))\n",
    "        # normalize mean square error so alpha is not dependent on dataset range\n",
    "        return (1 - self.alpha)*self.mse/self.dataset.range**2 + self.alpha*self.reg\n",
    "\n",
    "    def _reg(self):\n",
    "        user_graph = self.dataset.side_info.get('user_graph', None)\n",
    "        item_graph = self.dataset.side_info.get('item_graph', None)\n",
    "        \n",
    "        # compute L2 and graph regularization for all weights\n",
    "        self.reg_l2, self.reg_graph = 0, 0\n",
    "        for w in self.weights.values(): #将正则化的用户图和项目图相加\n",
    "            self.reg_l2 += tf.reduce_sum(w**2)\n",
    "            if w.shape[0] == self.dataset.n_user:\n",
    "                self.reg_graph += self._graph_reg(user_graph, w)\n",
    "            elif w.shape[0] == self.dataset.n_item:\n",
    "                self.reg_graph += self._graph_reg(item_graph, w)\n",
    "            else:\n",
    "                self.reg_graph += self._graph_reg(None, w) \n",
    "        \n",
    "        # beta hyperparameter controls L2 vs graph reg, range 0 to 1.\n",
    "        self.beta = float(self.config.get('beta', 0))\n",
    "        return (1 - self.beta)*self.reg_l2 + self.beta*self.reg_graph\n",
    "\n",
    "    def _graph_reg(self, g, w):\n",
    "        if g is None:\n",
    "            # if no graph is provided, use L2 reg\n",
    "            return tf.reduce_sum(w**2)\n",
    "        if len(w.shape) == 1:\n",
    "            w = tf.reshape(w, (-1, 1))\n",
    "        # normed controls whether or not to use normed graph laplacian\n",
    "        normed = self.config.get('normed', False)\n",
    "        # sparse controls whether or not to use sparse tensors\n",
    "        if self.config.get('sparse', True):\n",
    "            s = laplacian(sp.coo_matrix(g), normed=normed).astype(np.float32)\n",
    "            s = tf.sparse.reorder(tf.SparseTensor(np.array([s.row, s.col]).T, s.data, s.shape))\n",
    "            return tf.linalg.trace(tf.matmul(w, tf.sparse.matmul(s, w), transpose_a=True))\n",
    "        s = tf.constant(laplacian(g.A if sp.issparse(g) else g, normed=normed), dtype=tf.float32)\n",
    "        return tf.linalg.trace(tf.matmul(w, tf.matmul(s, w), transpose_a=True))\n",
    "\n",
    "    def run(self, ops, batch):\n",
    "        return self.session.run(ops, {\n",
    "            self.id: batch.index,\n",
    "            self.user_id: batch.user_id,\n",
    "            self.item_id: batch.item_id,\n",
    "            self.r_true: batch.rating,\n",
    "        })\n",
    "\n",
    "    def train(self, max_updates=100000, n_check=100, patience=float('inf'), batch_size=None):\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        best = {'updates': 0, 'loss': float('inf'),  'rmse_tune': float('inf')}\n",
    "        for i in range(max_updates):\n",
    "            # update\n",
    "            opt, loss = self.run([self.opt, self.loss], self.dataset.get_batch(mode='train', size=batch_size))\n",
    "            if i % n_check == 0 or i == max_updates - 1:\n",
    "                # monitoring\n",
    "                rmse_tune = self.run(self.rmse, self.dataset.get_batch(mode='tune', size=None))\n",
    "                if len(self.dataset.tune) == 0 or rmse_tune < best['rmse_tune']:\n",
    "                    rmse_test = self.run(self.rmse, self.dataset.get_batch(mode='test', size=None))\n",
    "                    best = {'updates': i, 'loss': loss, 'rmse_tune': rmse_tune, 'rmse_test': rmse_test}\n",
    "                print(best)\n",
    "                if (i - best['updates'])//n_check > patience:\n",
    "                    # early stopping\n",
    "                    break\n",
    "        return best\n",
    "    \n",
    "    def test(self):\n",
    "        return self._metrics(self.dataset.get_batch(mode='test', size=None))\n",
    "   \n",
    "    def _metrics(self, batch):\n",
    "        r_pred = self.run(self.r_pred, batch)\n",
    "        r_true, user_ids, item_ids = batch[['rating', 'user_id', 'item_id']].values.T\n",
    "        return {\n",
    "            'rmse': metrics.bootstrap(metrics.rmse, r_pred, r_true, user_ids, item_ids),\n",
    "#             'mae': metrics.bootstrap(metrics.mae, r_pred, r_true, user_ids, item_ids),\n",
    "#             'spearman': metrics.bootstrap(metrics.spearman, r_pred, r_true, user_ids, item_ids),\n",
    "#             'fcp': metrics.bootstrap(metrics.fcp, r_pred, r_true, user_ids, item_ids),\n",
    "#             'bpr': metrics.bootstrap(metrics.bpr, r_pred, r_true, user_ids, item_ids),\n",
    "       }\n",
    "\n",
    "    def _schema(self):\n",
    "        # parameters to save\n",
    "        return {\n",
    "            'weights': self.weights,\n",
    "            'biases': self.biases,\n",
    "        }\n",
    "\n",
    "    def _mask(self, g):\n",
    "        # define the mask used for GAT\n",
    "        if g is None:\n",
    "            # no masking\n",
    "            return 1.0\n",
    "        if self.config.get('sparse', True):\n",
    "            shape = g.shape\n",
    "            g = sp.coo_matrix(g, shape=shape, dtype=np.float32)\n",
    "            g = tf.sparse.reorder(tf.SparseTensor(np.array([g.row, g.col]).T, g.data, shape))\n",
    "            return tf.sparse.add(tf.sparse.eye(*shape), g)\n",
    "        return tf.eye(*g.shape) + tf.constant(g.A if sp.issparse(g) else g, dtype=tf.float32)\n",
    "\n",
    "    def _normalized_aggregation(self, g, w):\n",
    "        # define aggregation used for SVD++\n",
    "        if g is None:\n",
    "            return 0.0\n",
    "        if self.config.get('sparse', True):\n",
    "            g = sp.coo_matrix(g, dtype=np.float32)\n",
    "            g = tf.sparse.reorder(tf.SparseTensor(np.array([g.row, g.col]).T, g.data, g.shape))\n",
    "            return tf.sparse.matmul(g, w)/(tf.sparse.reduce_sum(g, axis=1, keepdims=True)**0.5 + 1e-10)\n",
    "        g = tf.constant(g.A if sp.issparse(g) else g, dtype=tf.float32)\n",
    "        return tf.matmul(g, w)/(tf.reduce_sum(g, axis=1, keepdims=True)**0.5 + 1e-10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "231a62f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVD(Model):\n",
    "    # GRALS: https://www.cs.utexas.edu/~rofuyu/papers/grmf-nips.pdf\n",
    "    def _params(self):\n",
    "        self.rank = int(self.config.get('rank', 1))\n",
    "        weights = {\n",
    "            'user_factor': tf.get_variable('user_factor', [self.dataset.n_user, self.rank]),\n",
    "            'item_factor': tf.get_variable('item_factor', [self.dataset.n_item, self.rank]),\n",
    "            'user_bias': tf.Variable(tf.zeros([self.dataset.n_user])),\n",
    "            'item_bias': tf.Variable(tf.zeros([self.dataset.n_item])),\n",
    "        }\n",
    "        biases = {'bias': tf.Variable(0.0)}\n",
    "        self.user_factor = tf.nn.embedding_lookup(weights['user_factor'], self.user_id)\n",
    "        self.item_factor = tf.nn.embedding_lookup(weights['item_factor'], self.item_id)\n",
    "        self.user_bias = tf.nn.embedding_lookup(weights['user_bias'], self.user_id)\n",
    "        self.item_bias = tf.nn.embedding_lookup(weights['item_bias'], self.item_id)\n",
    "        self.bias = biases['bias']\n",
    "        return weights, biases\n",
    "\n",
    "    def _r_pred(self):\n",
    "        return (tf.reduce_sum(self.user_factor*self.item_factor, 1)\n",
    "            + self.user_bias + self.item_bias + self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "246206d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVDpp(SVD):\n",
    "    # SVD++: https://people.engr.tamu.edu/huangrh/Spring16/papers_course/matrix_factorization.pdf\n",
    "    def _params(self):\n",
    "        self.config['beta'] = 0.0\n",
    "        weights, biases = super(SVDpp, self)._params()\n",
    "        weights['item_factor_pp'] = tf.get_variable('item_factor_pp', [self.dataset.n_item, self.rank])\n",
    "        g = sp.coo_matrix((self.dataset.data.is_train, (self.dataset.data.user_id, self.dataset.data.item_id)))\n",
    "        user_factor_pp = self._normalized_aggregation(g, weights['item_factor_pp'])\n",
    "        self.user_factor = tf.nn.embedding_lookup(weights['user_factor'] + user_factor_pp, self.user_id)\n",
    "        return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3753b3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MGCNN(Model):\n",
    "    # sRGCNN: https://github.com/fmonti/mgcnn\n",
    "    def _params(self):\n",
    "        self.config['normed'] = True\n",
    "        self.config['beta'] = 1.0\n",
    "        self.rank = int(self.config.get('rank', 1))\n",
    "        self.n_conv_feat = int(self.config.get('n_conv_feat', 32))\n",
    "        self.ord_row = int(self.config.get('ord_row', 5))\n",
    "        self.ord_col = int(self.config.get('ord_col', 5))\n",
    "        self.num_iterations = int(self.config.get('num_iterations', 10))\n",
    "\n",
    "        ##################################definition of the NN variables#####################################\n",
    "\n",
    "        #definition of the weights for extracting the global features\n",
    "        self.W_conv_W = tf.get_variable(\"W_conv_W\", shape=[self.ord_col*self.rank, self.n_conv_feat])\n",
    "        self.b_conv_W = tf.Variable(tf.zeros([self.n_conv_feat,]))\n",
    "        self.W_conv_H = tf.get_variable(\"W_conv_H\", shape=[self.ord_row*self.rank, self.n_conv_feat])\n",
    "        self.b_conv_H = tf.Variable(tf.zeros([self.n_conv_feat,]))\n",
    "\n",
    "        #recurrent N parameters\n",
    "        self.W_f_u = tf.get_variable(\"W_f_u\", shape=[self.n_conv_feat, self.n_conv_feat])\n",
    "        self.W_i_u = tf.get_variable(\"W_i_u\", shape=[self.n_conv_feat, self.n_conv_feat])\n",
    "        self.W_o_u = tf.get_variable(\"W_o_u\", shape=[self.n_conv_feat, self.n_conv_feat])\n",
    "        self.W_c_u = tf.get_variable(\"W_c_u\", shape=[self.n_conv_feat, self.n_conv_feat])\n",
    "        self.U_f_u = tf.get_variable(\"U_f_u\", shape=[self.n_conv_feat, self.n_conv_feat])\n",
    "        self.U_i_u = tf.get_variable(\"U_i_u\", shape=[self.n_conv_feat, self.n_conv_feat])\n",
    "        self.U_o_u = tf.get_variable(\"U_o_u\", shape=[self.n_conv_feat, self.n_conv_feat])\n",
    "        self.U_c_u = tf.get_variable(\"U_c_u\", shape=[self.n_conv_feat, self.n_conv_feat])\n",
    "        self.b_f_u = tf.Variable(tf.zeros([self.n_conv_feat,]))\n",
    "        self.b_i_u = tf.Variable(tf.zeros([self.n_conv_feat,]))\n",
    "        self.b_o_u = tf.Variable(tf.zeros([self.n_conv_feat,]))\n",
    "        self.b_c_u = tf.Variable(tf.zeros([self.n_conv_feat,]))\n",
    "\n",
    "        self.W_f_m = tf.get_variable(\"W_f_m\", shape=[self.n_conv_feat, self.n_conv_feat])\n",
    "        self.W_i_m = tf.get_variable(\"W_i_m\", shape=[self.n_conv_feat, self.n_conv_feat])\n",
    "        self.W_o_m = tf.get_variable(\"W_o_m\", shape=[self.n_conv_feat, self.n_conv_feat])\n",
    "        self.W_c_m = tf.get_variable(\"W_c_m\", shape=[self.n_conv_feat, self.n_conv_feat])\n",
    "        self.U_f_m = tf.get_variable(\"U_f_m\", shape=[self.n_conv_feat, self.n_conv_feat])\n",
    "        self.U_i_m = tf.get_variable(\"U_i_m\", shape=[self.n_conv_feat, self.n_conv_feat])\n",
    "        self.U_o_m = tf.get_variable(\"U_o_m\", shape=[self.n_conv_feat, self.n_conv_feat])\n",
    "        self.U_c_m = tf.get_variable(\"U_c_m\", shape=[self.n_conv_feat, self.n_conv_feat])\n",
    "        self.b_f_m = tf.Variable(tf.zeros([self.n_conv_feat,]))\n",
    "        self.b_i_m = tf.Variable(tf.zeros([self.n_conv_feat,]))\n",
    "        self.b_o_m = tf.Variable(tf.zeros([self.n_conv_feat,]))\n",
    "        self.b_c_m = tf.Variable(tf.zeros([self.n_conv_feat,]))\n",
    "\n",
    "        #output parameters\n",
    "        self.W_out_W = tf.get_variable(\"W_out_W\", shape=[self.n_conv_feat, self.rank]) \n",
    "        self.b_out_W = tf.Variable(tf.zeros([self.rank,]))\n",
    "        self.W_out_H = tf.get_variable(\"W_out_H\", shape=[self.n_conv_feat, self.rank]) \n",
    "        self.b_out_H = tf.Variable(tf.zeros([self.rank,]))\n",
    "\n",
    "        #########definition of the NN\n",
    "        #definition of W and H\n",
    "        d, shape = self.dataset.train, (self.dataset.n_user, self.dataset.n_item)\n",
    "        matrix_train = sp.coo_matrix((d.rating, (d.user_id, d.item_id)), shape=shape)\n",
    "        u, s, vt = svds(matrix_train.astype(float), k=self.rank)\n",
    "        self.W = tf.constant(u*s**0.5, dtype=tf.float32)\n",
    "        self.H = tf.constant(vt.T*s**0.5, dtype=tf.float32)\n",
    "\n",
    "        #RNN\n",
    "        self.h_u = tf.zeros([self.dataset.n_user, self.n_conv_feat])\n",
    "        self.c_u = tf.zeros([self.dataset.n_user, self.n_conv_feat])\n",
    "        self.h_m = tf.zeros([self.dataset.n_item, self.n_conv_feat])\n",
    "        self.c_m = tf.zeros([self.dataset.n_item, self.n_conv_feat])\n",
    "\n",
    "        for k in range(self.num_iterations):\n",
    "            #extraction of global features vectors\n",
    "            self.final_feat_users = self._mono_conv(\n",
    "                self.dataset.side_info['user_graph'], self.ord_row, self.W, self.W_conv_W, self.b_conv_W)\n",
    "            self.final_feat_movies = self._mono_conv(\n",
    "                self.dataset.side_info['item_graph'], self.ord_col, self.H, self.W_conv_H, self.b_conv_H)\n",
    "\n",
    "            #here we have to split the features between users and movies LSTMs\n",
    "\n",
    "            #users RNN\n",
    "            self.f_u = tf.sigmoid(tf.matmul(self.final_feat_users, self.W_f_u) + tf.matmul(self.h_u, self.U_f_u) + self.b_f_u)\n",
    "            self.i_u = tf.sigmoid(tf.matmul(self.final_feat_users, self.W_i_u) + tf.matmul(self.h_u, self.U_i_u) + self.b_i_u)\n",
    "            self.o_u = tf.sigmoid(tf.matmul(self.final_feat_users, self.W_o_u) + tf.matmul(self.h_u, self.U_o_u) + self.b_o_u)\n",
    "\n",
    "            self.update_c_u = tf.sigmoid(tf.matmul(self.final_feat_users, self.W_c_u) + tf.matmul(self.h_u, self.U_c_u) + self.b_c_u)\n",
    "            self.c_u = tf.multiply(self.f_u, self.c_u) + tf.multiply(self.i_u, self.update_c_u)\n",
    "            self.h_u = tf.multiply(self.o_u, tf.sigmoid(self.c_u))\n",
    "\n",
    "            #movies RNN\n",
    "            self.f_m = tf.sigmoid(tf.matmul(self.final_feat_movies, self.W_f_m) + tf.matmul(self.h_m, self.U_f_m) + self.b_f_m)\n",
    "            self.i_m = tf.sigmoid(tf.matmul(self.final_feat_movies, self.W_i_m) + tf.matmul(self.h_m, self.U_i_m) + self.b_i_m)\n",
    "            self.o_m = tf.sigmoid(tf.matmul(self.final_feat_movies, self.W_o_m) + tf.matmul(self.h_m, self.U_o_m) + self.b_o_m)\n",
    "\n",
    "            self.update_c_m = tf.sigmoid(tf.matmul(self.final_feat_movies, self.W_c_m) + tf.matmul(self.h_m, self.U_c_m) + self.b_c_m)\n",
    "            self.c_m = tf.multiply(self.f_m, self.c_m) + tf.multiply(self.i_m, self.update_c_m)\n",
    "            self.h_m = tf.multiply(self.o_m, tf.sigmoid(self.c_m))\n",
    "\n",
    "            #compute update of matrix X\n",
    "            self.delta_W = tf.tanh(tf.matmul(self.c_u, self.W_out_W) + self.b_out_W) #N x rank_W_H\n",
    "            self.delta_H = tf.tanh(tf.matmul(self.c_m, self.W_out_H) + self.b_out_H) #M x rank_W_H\n",
    "\n",
    "            self.W += self.delta_W\n",
    "            self.H += self.delta_H\n",
    "            \n",
    "        self.user_factor = tf.nn.embedding_lookup(self.W, self.user_id)\n",
    "        self.item_factor = tf.nn.embedding_lookup(self.H, self.item_id)\n",
    "        \n",
    "        return {'W': self.W, 'H': self.H}, {}\n",
    "\n",
    "    def _r_pred(self):\n",
    "        return tf.reduce_sum(self.user_factor*self.item_factor, 1)\n",
    "\n",
    "    def _mono_conv(self, g, ord_conv, A, W, b):\n",
    "        if self.config.get('sparse', True):\n",
    "            L = sp.coo_matrix(laplacian(g, normed=True) - sp.eye(*g.shape), dtype=np.float32)\n",
    "            L = tf.sparse.reorder(tf.SparseTensor(np.array([L.row, L.col]).T, L.data, L.shape))\n",
    "            matmul = tf.sparse.matmul\n",
    "        else:\n",
    "            L = tf.constant(laplacian(g, normed=True) - np.eye(*g.shape), dtype=tf.float32)\n",
    "            matmul = tf.matmul\n",
    "        feat = []\n",
    "        for k in range(ord_conv):\n",
    "            if k == 0:\n",
    "                feat.append(A)\n",
    "            elif k == 1:\n",
    "                feat.append(matmul(L, A))\n",
    "            else:\n",
    "                feat.append(matmul(L, 2*feat[k - 1]) - feat[k - 2])\n",
    "        return tf.nn.relu(tf.matmul(tf.concat(feat, 1), W) + b)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be6fc768",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATSVD(SVD):\n",
    "    # MG-GAT: our model\n",
    "    def _params(self):\n",
    "        weights, biases = super(GATSVD, self)._params()\n",
    "        data, shape = self.dataset.data, (self.dataset.n_user, self.dataset.n_item)\n",
    "        implicit = sp.coo_matrix((data.is_train, (data.user_id, data.item_id)), shape=shape)\n",
    "        self.k = int(self.config.get('k', 1))\n",
    "        u, s, vt = svds(implicit.astype(float), k=self.k)\n",
    "        self.user_features = tf.constant(u*s**0.5, dtype=tf.float32)\n",
    "        self.item_features = tf.constant(vt.T*s**0.5, dtype=tf.float32)\n",
    "        user_features = self.dataset.side_info.get('user_features', None)\n",
    "        item_features = self.dataset.side_info.get('item_features', None)\n",
    "        if user_features is not None:\n",
    "            self.user_features = tf.concat(\n",
    "                [self.user_features, tf.constant(user_features.values, dtype=tf.float32)], 1)\n",
    "        if item_features is not None:\n",
    "            self.item_features = tf.concat(\n",
    "                [self.item_features, tf.constant(item_features.values, dtype=tf.float32)], 1)\n",
    "        self.n_head = int(self.config.get('n_head', 1))\n",
    "        self.activation_in = tf.keras.activations.get(self.config.get('activation_in', 'softsign'))\n",
    "        self.activation_out = tf.keras.activations.get(self.config.get('activation_out', 'hard_sigmoid'))\n",
    "        self.residual = bool(self.config.get('residual', True))\n",
    "        self.user_in = layers.GAT(\n",
    "            self.user_features.shape[1], self.n_head*self.rank, 1, concat=False, residual=self.residual, name='user_in')\n",
    "        self.user_out = layers.Dense(self.user_in.dim_out, self.rank, name='user_out')\n",
    "        self.item_in = layers.GAT(\n",
    "            self.item_features.shape[1], self.n_head*self.rank, 1, concat=False, residual=self.residual, name='item_in')\n",
    "        self.item_out = layers.Dense(self.item_in.dim_out, self.rank, name='item_out')\n",
    "        for layer in [self.user_in, self.user_out, self.item_in, self.item_out]:\n",
    "            weights.update(layer.get_weights())\n",
    "            biases.update(layer.get_biases())\n",
    "        self.user_mask = self._mask(self.dataset.side_info.get('user_graph', None))\n",
    "        self.item_mask = self._mask(self.dataset.side_info.get('item_graph', None))\n",
    "        sparse = bool(self.config.get('sparse', True))\n",
    "        self.user_factor_pp = self.user_out(self.user_in(\n",
    "            self.user_features, self.activation_in, self.user_mask, sparse), self.activation_out)\n",
    "        self.item_factor_pp = self.item_out(self.item_in(\n",
    "            self.item_features, self.activation_in, self.item_mask, sparse), self.activation_out)\n",
    "        self.user_factor = tf.nn.embedding_lookup(weights['user_factor'] + self.user_factor_pp, self.user_id)\n",
    "        self.item_factor = tf.nn.embedding_lookup(weights['item_factor'] + self.item_factor_pp, self.item_id)\n",
    "        return weights, biases\n",
    "\n",
    "    def _schema(self):\n",
    "        return {\n",
    "            'weights': self.weights,\n",
    "            'biases': self.biases,\n",
    "            'outputs': {\n",
    "                'user_alpha': self.user_in.heads[0].alpha,\n",
    "                'item_alpha': self.item_in.heads[0].alpha,\n",
    "                'user_factor_pp': self.user_factor_pp,\n",
    "                'item_factor_pp': self.item_factor_pp,\n",
    "                'user_factor': self.weights['user_factor'] + self.user_factor_pp,\n",
    "                'item_factor': self.weights['item_factor'] + self.item_factor_pp,\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f400a729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 4.000: None | Iter 1.000: None<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.92 GiB heap, 0.0/2.46 GiB objects<br>Result logdir: /Users/liujiaoyang/tensorflow/MGGAT/data/results/ray_results/MGCNN_Douban<br>Number of trials: 1/500 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th>activation_in  </th><th>activation_out  </th><th style=\"text-align: right;\">      alpha</th><th style=\"text-align: right;\">      beta</th><th style=\"text-align: right;\">  k</th><th style=\"text-align: right;\">  n_head</th><th style=\"text-align: right;\">  rank</th><th>residual  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>validation_0bddf3ea</td><td>PENDING </td><td>     </td><td>sigmoid        </td><td>hard_sigmoid    </td><td style=\"text-align: right;\">0.000420987</td><td style=\"text-align: right;\">0.00199531</td><td style=\"text-align: right;\">  3</td><td style=\"text-align: right;\">       2</td><td style=\"text-align: right;\">     3</td><td>True      </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 4.000: None | Iter 1.000: None<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.92 GiB heap, 0.0/2.46 GiB objects<br>Result logdir: /Users/liujiaoyang/tensorflow/MGGAT/data/results/ray_results/MGCNN_Douban<br>Number of trials: 1/500 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th>activation_in  </th><th>activation_out  </th><th style=\"text-align: right;\">      alpha</th><th style=\"text-align: right;\">      beta</th><th style=\"text-align: right;\">  k</th><th style=\"text-align: right;\">  n_head</th><th style=\"text-align: right;\">  rank</th><th>residual  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>validation_0bddf3ea</td><td>PENDING </td><td>     </td><td>sigmoid        </td><td>hard_sigmoid    </td><td style=\"text-align: right;\">0.000420987</td><td style=\"text-align: right;\">0.00199531</td><td style=\"text-align: right;\">  3</td><td style=\"text-align: right;\">       2</td><td style=\"text-align: right;\">     3</td><td>True      </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-05 19:06:34,566\tWARNING tune.py:506 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 4.000: None | Iter 1.000: None<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/4.92 GiB heap, 0.0/2.46 GiB objects<br>Result logdir: /Users/liujiaoyang/tensorflow/MGGAT/data/results/ray_results/MGCNN_Douban<br>Number of trials: 1/500 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th>activation_in  </th><th>activation_out  </th><th style=\"text-align: right;\">      alpha</th><th style=\"text-align: right;\">      beta</th><th style=\"text-align: right;\">  k</th><th style=\"text-align: right;\">  n_head</th><th style=\"text-align: right;\">  rank</th><th>residual  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>validation_0bddf3ea</td><td>PENDING </td><td>     </td><td>sigmoid        </td><td>hard_sigmoid    </td><td style=\"text-align: right;\">0.000420987</td><td style=\"text-align: right;\">0.00199531</td><td style=\"text-align: right;\">  3</td><td style=\"text-align: right;\">       2</td><td style=\"text-align: right;\">     3</td><td>True      </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-05 19:06:34,783\tERROR tune.py:545 -- Trials did not complete: [validation_0bddf3ea]\n",
      "2021-07-05 19:06:34,784\tINFO tune.py:549 -- Total run time: 6.35 seconds (6.11 seconds for the tuning loop).\n",
      "2021-07-05 19:06:34,786\tWARNING tune.py:553 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "No trials found in /Users/liujiaoyang/tensorflow/MGGAT/data/results/ray_results/MGCNN_Douban.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-73e5cfeac3b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m             num_samples=N_SAMPLES)\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0manalysis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAnalysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRAY_RESULTS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/tune/analysis/experiment_analysis.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, experiment_dir, default_metric, default_mode)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \"Analysis utilities.\")\n\u001b[1;32m     70\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_trial_dataframes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/tune/analysis/experiment_analysis.py\u001b[0m in \u001b[0;36mfetch_trial_dataframes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch_trial_dataframes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mfail_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_trial_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 self.trial_dataframes[path] = pd.read_csv(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/tune/analysis/experiment_analysis.py\u001b[0m in \u001b[0;36m_get_trial_paths\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_trial_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             raise TuneError(\"No trials found in {}.\".format(\n\u001b[0m\u001b[1;32m    333\u001b[0m                 self._experiment_dir))\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_trial_paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: No trials found in /Users/liujiaoyang/tensorflow/MGGAT/data/results/ray_results/MGCNN_Douban."
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Arguments\n",
    "    MODEL = MGCNN # SVD, SVDpp, MGCNN, or GATSVD\n",
    "    CWD = os.getcwd()\n",
    "    DATASET_PATH = CWD + '/data/datasets/Douban' # check data/datasets for options\n",
    "    METRICS_PATH = CWD + '/data/results/metrics'\n",
    "    RAY_RESULTS = CWD + '/data/results/ray_results'\n",
    "    BATCH_SIZE = 300 # change to integer to use minibatches\n",
    "    TUNE = True # False to use saved hyperparameters, True to search\n",
    "    N_SAMPLES = 500 # number of hyperparameter sets to try if TUNE == True\n",
    "    ACTIVATIONS = [\n",
    "        'elu', 'exponential', 'hard_sigmoid', 'linear', 'relu',\n",
    "        'selu', 'sigmoid', 'softplus', 'softsign', 'tanh',\n",
    "    ]\n",
    "    DATASET = Dataset.load(DATASET_PATH)\n",
    "    NAME = '{}_{}'.format(MODEL.__name__, DATASET.name)\n",
    "    N_FOLDS = 10 if len(DATASET.tune) == 0 else 1 # 10-fold cross validation\n",
    "    \n",
    "    \n",
    "    if TUNE:\n",
    "        def validation(config):\n",
    "            DATASET = Dataset.load(DATASET_PATH)\n",
    "            if len(DATASET.tune) == 0:\n",
    "                FOLDS = np.random.randint(N_FOLDS, size=DATASET.n_data - len(DATASET.test))\n",
    "\n",
    "            # add DATASET.max so np.mean(metrics['rmse_tune']) decreases with each iteration\n",
    "            # which is necessary for AsyncHyperBandScheduler\n",
    "            metrics = {'updates': [], 'rmse': [DATASET.max]}\n",
    "            for j in range(N_FOLDS):\n",
    "                data = DATASET.data[['user_id', 'item_id', 'rating', 'is_test', 'is_tune']]\n",
    "                print('1')\n",
    "                if len(DATASET.tune) == 0:\n",
    "                    data.loc[data.is_test == False, 'is_tune'] = FOLDS == j\n",
    "                dataset = Dataset(data, **DATASET.side_info)\n",
    "                print('2')\n",
    "                with tf.Graph().as_default():\n",
    "                    with tf.Session() as session:\n",
    "                        model = MODEL(session, dataset, **config)\n",
    "                        best = model.train(patience=10, batch_size=BATCH_SIZE)\n",
    "                print('3')\n",
    "                metrics['updates'].append(best['updates'])\n",
    "                metrics['rmse'].append(best['rmse_tune'])\n",
    "                tune.report(updates=np.mean(metrics['updates']), rmse=np.mean(metrics['rmse']))\n",
    "        \n",
    "        tune.run(\n",
    "            validation,\n",
    "            name=NAME,\n",
    "            search_alg=HyperOptSearch({\n",
    "                    'alpha': hp.loguniform('alpha', -20, 0),\n",
    "                    'beta': hp.loguniform('beta', -20, 0),\n",
    "                    'rank': hp.qloguniform('rank', 0, 5, 1),\n",
    "                    'k': hp.qloguniform('k', 0, 3, 1),\n",
    "                    'n_head': hp.qloguniform('n_head', 0, 2, 1),\n",
    "                    'activation_in': hp.choice('activation_in', ACTIVATIONS),\n",
    "                    'activation_out': hp.choice('activation_out', ACTIVATIONS),\n",
    "                    'residual': hp.choice('residual', [True, False]),\n",
    "                }, metric='rmse', mode='min'),\n",
    "            scheduler=AsyncHyperBandScheduler(metric='rmse', mode='min', max_t=N_FOLDS),\n",
    "            resources_per_trial={\"gpu\": 1},\n",
    "            local_dir=RAY_RESULTS,\n",
    "            num_samples=N_SAMPLES)\n",
    "\n",
    "    analysis = tune.Analysis('{}/{}'.format(RAY_RESULTS, NAME))\n",
    "    config = analysis.get_best_config(metric='rmse', mode='min')\n",
    "    df = analysis.dataframe()\n",
    "    config['max_updates'] = int(df[df['config/alpha'] == config['alpha']]['updates'])\n",
    "    print(config)\n",
    "\n",
    "    data = DATASET.data[['user_id', 'item_id', 'rating', 'is_test']]\n",
    "    dataset = Dataset(data, **DATASET.side_info)\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.Session() as session:\n",
    "            model = MODEL(session, dataset, **config)\n",
    "            model.train(max_updates=config['max_updates'], batch_size=BATCH_SIZE)\n",
    "            best = pd.DataFrame(model.test())\n",
    "            print(best.describe())\n",
    "            best.to_csv('{}/{}.csv'.format(METRICS_PATH, NAME), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3589af98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0c217e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
