{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbf0c508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "703495c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(object):\n",
    "    def __init__(self, dim_in, dim_out, name='dense'):\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        with tf.variable_scope(name): #规定变量名的作用域\n",
    "            self.kernel = tf.get_variable('kernel', [dim_in, dim_out]) #创建变量kernel\n",
    "            self.bias = tf.Variable(tf.zeros([dim_out]), name='bias') #初始化bias为dim_out维的0向量\n",
    "    \n",
    "    def __call__(self, inputs, activation=lambda a: a): # 接受参数，执行表达式，返回结果 -- 返回a的值\n",
    "        return activation(tf.matmul(inputs, self.kernel) + self.bias) #将输入与kernel相乘加上bias\n",
    "\n",
    "    def get_weights(self):\n",
    "        return {self.kernel.name: self.kernel} #返回kernel\n",
    "\n",
    "    def get_biases(self):\n",
    "        return {self.bias.name: self.bias} #返回bias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "869b60af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(Dense):\n",
    "    # https://tkipf.github.io/graph-convolutional-networks/\n",
    "    # https://arxiv.org/pdf/1609.02907.pdf\n",
    "    def __init__(self, dim_in, dim_out, name='gcn'):\n",
    "        super(GCN, self).__init__(dim_in, dim_out, name=name)\n",
    "    \n",
    "    def __call__(self, inputs, activation=lambda a: a, mask=1.0, sparse=False):\n",
    "        if sparse and mask != 1.0:\n",
    "            self.degree = tf.sparse.reduce_sum(mask, axis=1, keepdims=True)**-0.5 \n",
    "            #reduce_sum:压缩求和，用于降维\n",
    "            #按行求和，且保留维度\n",
    "            self.alpha = mask*self.degree*tf.transpose(self.degree) \n",
    "            #transpose:转置\n",
    "            #将mask的变成self.degree*self.degree维矩阵\n",
    "            return activation(tf.sparse.matmul(self.alpha, tf.matmul(inputs, self.kernel)) + self.bias)\n",
    "        self.degree = tf.reduce_sum(mask, axis=1, keepdims=True)**-0.5\n",
    "        self.alpha = mask*self.degree*tf.transpose(self.degree)\n",
    "        return activation(tf.matmul(self.alpha, tf.matmul(inputs, self.kernel)) + self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "186b70b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATHead(Dense):\n",
    "    # https://github.com/PetarV-/GAT/blob/master/utils/layers.py\n",
    "    def __init__(self, dim_in, dim_out, residual=False, name='gathead'):\n",
    "        #super:调用父类\n",
    "        super(GATHead, self).__init__(dim_in, dim_out, name=name)\n",
    "        with tf.variable_scope(name):\n",
    "            self.attention_self = Dense(dim_out, 1, name='self')\n",
    "            self.attention_other = Dense(dim_out, 1, name='other')\n",
    "            self.residual = Dense(dim_in, dim_out, name='residual') if residual else None #残差\n",
    "\n",
    "    def __call__(self, inputs, activation=lambda a: a, mask=1.0, sparse=False):\n",
    "        x = tf.matmul(inputs, self.kernel)\n",
    "        u, v = self.attention_self(x), self.attention_other(x)\n",
    "        if sparse and mask != 1.0:\n",
    "            logits = tf.sparse.add(mask*u, mask*tf.transpose(v))\n",
    "            logits = tf.SparseTensor(logits.indices, tf.nn.leaky_relu(logits.values), logits.shape)\n",
    "            #线性整流函数\n",
    "            #tf.nn.relu()函数的目的是，将输入小于0的值幅值为0，输入大于0的值不变。\n",
    "            self.alpha = tf.sparse.softmax(logits)\n",
    "            #将 softmax 应用于批量的 N 维 SparseTensor.\n",
    "            #softmax：归一化指数函数\n",
    "            out = tf.sparse.matmul(self.alpha, x) + self.bias\n",
    "        else:\n",
    "            logits = tf.nn.leaky_relu(u + tf.transpose(v))\n",
    "            self.alpha = mask*tf.exp(logits - tf.reduce_max(logits, 1)) #降维，按行取最大值\n",
    "            self.alpha /= tf.reduce_sum(self.alpha, 1, keepdims=True) #与上一行一起相当于归一化指数函数\n",
    "            out = tf.matmul(self.alpha, x) + self.bias\n",
    "        return activation(out if self.residual is None else out + self.residual(inputs))\n",
    "\n",
    "    def get_weights(self): \n",
    "        weights = {\n",
    "            self.kernel.name: self.kernel,\n",
    "            self.attention_self.kernel.name: self.attention_self.kernel,\n",
    "            self.attention_other.kernel.name: self.attention_other.kernel,\n",
    "        }\n",
    "        if self.residual is not None:\n",
    "            weights[self.residual.kernel.name] = self.residual.kernel\n",
    "        return weights\n",
    "\n",
    "    def get_biases(self):\n",
    "        biases = {\n",
    "            self.bias.name: self.bias,\n",
    "            self.attention_self.bias.name: self.attention_self.bias,\n",
    "            self.attention_other.bias.name: self.attention_other.bias,\n",
    "        }\n",
    "        if self.residual is not None:\n",
    "            biases[self.residual.bias.name] = self.residual.bias\n",
    "        return biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3e5e2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(object):\n",
    "    # https://arxiv.org/pdf/1710.10903.pdf\n",
    "    def __init__(self, dim_in, dim_out, n_head, residual=False, concat=True, name='gat'):\n",
    "        with tf.variable_scope(name):\n",
    "            self.heads = [GATHead(dim_in, dim_out, residual=residual, name=str(i)) for i in range(n_head)]\n",
    "        self.dim_in = dim_in\n",
    "        self.n_head = n_head\n",
    "        self.concat = concat\n",
    "        self.dim_out = dim_out*n_head if concat else dim_out\n",
    "\n",
    "    def __call__(self, inputs, activation=lambda a: a, mask=1.0, sparse=False):\n",
    "        if self.concat:\n",
    "            return tf.concat([h(inputs, activation=activation, mask=mask, sparse=sparse) for h in self.heads], 1)\n",
    "        return activation(tf.reduce_mean([h(inputs, mask=mask, sparse=sparse) for h in self.heads], 0))\n",
    "\n",
    "    def get_weights(self):\n",
    "        weights = {}\n",
    "        for h in self.heads:\n",
    "            weights.update(h.get_weights())\n",
    "        return weights\n",
    "\n",
    "    def get_biases(self):\n",
    "        biases = {}\n",
    "        for h in self.heads:\n",
    "            biases.update(h.get_biases())\n",
    "        return biases\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b2b2e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
