{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdee3063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from ast import literal_eval\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "549baff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, data, **side_info):\n",
    "        data = pd.DataFrame(data, copy=True)\n",
    "        if 'is_test' not in data.columns:\n",
    "            data['is_test'] = False\n",
    "        if 'is_tune' not in data.columns:\n",
    "            data['is_tune'] = False\n",
    "        data = data[['user_id', 'item_id', 'rating', 'is_test', 'is_tune']].astype({\n",
    "            'user_id': int, 'item_id': int, 'rating': float, 'is_test': bool, 'is_tune': bool,\n",
    "        })\n",
    "        data['is_train'] = (data.is_test == False) & (data.is_tune == False)\n",
    "        \n",
    "        self.train = data[data.is_train == True].sample(frac=1)\n",
    "        self.tune = data[data.is_tune == True].sample(frac=1)\n",
    "        self.test = data[data.is_test == True].sample(frac=1)\n",
    "        self.index = {'train': 0, 'tune': 0, 'test': 0}\n",
    "        \n",
    "        self.n_data = len(data) #数据长度\n",
    "        self.min = data['rating'].min() #数据最小值\n",
    "        self.max = data['rating'].max() #数据最大值\n",
    "        self.range = self.max - self.min #数据范围\n",
    "        self.data = data\n",
    "        \n",
    "        self.name = side_info['name'] = side_info.get('name', 'dataset') #数据集名称\n",
    "        self.n_user = side_info['n_user'] = side_info.get('n_user', self.data['user_id'].max() + 1) #用户数\n",
    "        self.n_item = side_info['n_item'] = side_info.get('n_item', self.data['item_id'].max() + 1) #项目数\n",
    "        self.shape = (self.n_user, self.n_item) #矩阵长宽，行为用户， 列为项目\n",
    "        self.side_info = side_info\n",
    "\n",
    "    def get_batch(self, mode='train', size=None): #获取数据块\n",
    "        dataset = getattr(self, mode) #获取对象属性值\n",
    "        if size is None:\n",
    "            return dataset[:]\n",
    "        i = self.index[mode]\n",
    "        self.index[mode] = i + size if i + size < len(dataset) else 0\n",
    "        return dataset[i:(i + size)]\n",
    "\n",
    "    def save(self, path): #存储\n",
    "        folder = '{}/{}'.format(path, self.name)\n",
    "        if not os.path.exists(folder): #创建存储路径\n",
    "            os.makedirs(folder)\n",
    "        self.data.to_csv(folder + '/data.csv', index=False)\n",
    "        \n",
    "        metadata = pd.DataFrame({'name': [self.name], 'n_user': [self.n_user], 'n_item': [self.n_item]})\n",
    "        metadata.to_csv(folder + '/metadata.csv', index=False) #创建metadata.csv\n",
    "        \n",
    "        for k, v in self.side_info.items():\n",
    "            if sp.issparse(v):\n",
    "                sp.save_npz('{}/{}.npz'.format(folder, k), v)\n",
    "            elif isinstance(v, np.ndarray):\n",
    "                np.save('{}/{}.npy'.format(folder, k), v)\n",
    "            elif isinstance(v, pd.DataFrame):\n",
    "                v.to_csv('{}/{}.csv'.format(folder, k), index=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path, **kwargs):\n",
    "        kwargs['name'] = os.path.basename(path)\n",
    "        for f in os.listdir(path):\n",
    "            if f.startswith('.'):\n",
    "                continue\n",
    "            f_path = '{}/{}'.format(path, f)\n",
    "            if f.endswith('metadata.csv'):\n",
    "                s = pd.read_csv(f_path).iloc[0]\n",
    "                kwargs.update(s.to_dict())\n",
    "            elif f.endswith('.npz'):\n",
    "                kwargs[f[:-4]] = sp.load_npz(f_path)\n",
    "            elif f.endswith('.npy'):\n",
    "                kwargs[f[:-4]] = np.load(f_path)\n",
    "            elif f.endswith('.csv'):\n",
    "                kwargs[f[:-4]] = pd.read_csv(f_path)\n",
    "            else:\n",
    "                print('File not loaded: ' + f)\n",
    "        return Dataset(**kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01a99d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Monti(Dataset):\n",
    "    def __init__(self, path, **kwargs):\n",
    "        matrix = sp.coo_matrix(self._load_matlab_file(path, 'M'))\n",
    "        data = pd.DataFrame({'user_id': matrix.row, 'item_id': matrix.col, 'rating': matrix.data})\n",
    "        mask_test = sp.dok_matrix(self._load_matlab_file(path, 'Otest'))\n",
    "        data['is_test'] = [mask_test[i, j] for i, j in zip(matrix.row, matrix.col)]\n",
    "        \n",
    "        kwargs['data'] = data\n",
    "        kwargs['name'] = kwargs.get('name', 'monti')\n",
    "        kwargs['n_user'] = kwargs.get('n_user', 3000)\n",
    "        kwargs['n_item'] = kwargs.get('n_item', 3000)\n",
    "        super(Monti, self).__init__(**kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def _load_matlab_file(cls, path_file, name_field):\n",
    "        # https://github.com/fmonti/mgcnn\n",
    "        \"\"\"\n",
    "        load '.mat' files\n",
    "        inputs:\n",
    "            path_file, string containing the file path\n",
    "            name_field, string containig the field name (default='shape')\n",
    "        warning:\n",
    "            '.mat' files should be saved in the '-v7.3' format\n",
    "        \"\"\"\n",
    "        db = h5py.File(path_file, 'r')\n",
    "        ds = db[name_field]\n",
    "        try:\n",
    "            if 'ir' in ds.keys():\n",
    "                data = np.asarray(ds['data'])\n",
    "                ir   = np.asarray(ds['ir'])\n",
    "                jc   = np.asarray(ds['jc'])\n",
    "                out  = sp.csc_matrix((data, ir, jc)).astype(np.float32)\n",
    "        except AttributeError:\n",
    "            # Transpose in case is a dense matrix because of the row- vs column- major ordering between python and matlab\n",
    "            out = np.asarray(ds).astype(np.float32).T\n",
    "        db.close()\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e13f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Douban(Monti):\n",
    "    def __init__(self, path='/Users/liujiaoyang/tensorflow/MGGAT/data/raw_data/mgcnn/douban/training_test_dataset.mat', **kwargs):\n",
    "        kwargs['name'] = kwargs.get('name', 'Douban')\n",
    "        kwargs['user_graph'] = sp.coo_matrix(self._load_matlab_file(path, 'W_users'))\n",
    "        super(Douban, self).__init__(path, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a306dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flixster(Monti):\n",
    "    def __init__(self, path='/Users/liujiaoyang/tensorflow/MGGAT/data/raw_data/mgcnn/flixster/training_test_dataset_10_NNs.mat', **kwargs):\n",
    "        kwargs['name'] = kwargs.get('name', 'Flixster')\n",
    "        kwargs['user_graph'] = sp.coo_matrix(self._load_matlab_file(path, 'W_users'))\n",
    "        kwargs['item_graph'] = sp.coo_matrix(self._load_matlab_file(path, 'W_movies'))\n",
    "        super(Flixster, self).__init__(path, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a421a885",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Movielens(Monti):\n",
    "    def __init__(self, path='/Users/liujiaoyang/tensorflow/MGGAT/data/raw_data/mgcnn/movielens/split_1.mat', **kwargs):\n",
    "        kwargs['name'] = kwargs.get('name', 'Movielens')\n",
    "        kwargs['user_graph'] = sp.coo_matrix(self._load_matlab_file(path, 'W_users'))\n",
    "        kwargs['item_graph'] = sp.coo_matrix(self._load_matlab_file(path, 'W_movies'))\n",
    "        kwargs['n_user'] = kwargs['user_graph'].shape[0]\n",
    "        kwargs['n_item'] = kwargs['item_graph'].shape[0]\n",
    "        super(Movielens, self).__init__(path, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324dfafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YahooMusic(Monti):\n",
    "    def __init__(self, path='/Users/liujiaoyang/tensorflow/MGGAT/data/raw_data/mgcnn/yahoo_music/training_test_dataset_10_NNs.mat', **kwargs): \n",
    "        kwargs['name'] = kwargs.get('name', 'YahooMusic')\n",
    "        kwargs['item_graph'] = sp.coo_matrix(self._load_matlab_file(path, 'W_tracks'))\n",
    "        super(YahooMusic, self).__init__(path, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4befad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLens100K(Movielens):\n",
    "    def __init__(self, path='/Users/liujiaoyang/tensorflow/MGGAT/data/raw_data/ml-100k', **kwargs):        \n",
    "        user_columns = ['user_id', 'age', 'gender', 'occupation', 'zip_code']\n",
    "        user = pd.read_csv(path + '/u.user', engine='python', encoding='ISO-8859-1', sep='|', names=user_columns)\n",
    "        user.age = user.age.astype(float)/user.age.max()\n",
    "        user.gender = user.gender.astype('category').cat.codes\n",
    "        user = pd.get_dummies(user, columns=['occupation'])\n",
    "        user.drop(columns=['user_id', 'zip_code'], inplace=True)\n",
    "        kwargs['user_features'] = user.astype(float)\n",
    "\n",
    "        item_columns = [\n",
    "            'movie_id', 'movie_title', 'release_date', 'video_release_date', 'IMDb_URL', 'unknown',\n",
    "            'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary',\n",
    "            'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi',\n",
    "            'Thriller', 'War', 'Western',\n",
    "        ]\n",
    "        item = pd.read_csv(path + '/u.item', engine='python', encoding='ISO-8859-1', sep='|', names=item_columns)\n",
    "        item.drop(columns=item_columns[:6], inplace=True)\n",
    "        kwargs['item_features'] = item.astype(float)\n",
    "\n",
    "        kwargs['name'] = 'MovieLens100K'\n",
    "        super(MovieLens100K, self).__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "662975a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Yelp2021(Dataset):\n",
    "    def __init__(self, state, path='/Users/liujiaoyang/tensorflow/MGGAT/data/raw_data/yelp2021', **kwargs):\n",
    "        # **表示将dict字典类型的数据作为参数传入，kwargs--keyword argument\n",
    "        kwargs['name'] = state.upper().replace(' ', '') #把字符串中的单引号去掉\n",
    "        \n",
    "        print('loading user data...')\n",
    "        user = pd.read_json(path + '/user.json', lines=True)\n",
    "        print('over')\n",
    "        print('loading business data...')\n",
    "        item = pd.read_json(path + '/business.json', lines=True).rename(columns={'business_id': 'item_id'}) #修改列的名称\n",
    "        print('loading review data...')\n",
    "        data = pd.read_json(path + '/review.json', lines=True).rename(columns={'business_id': 'item_id', 'stars': 'rating'})\n",
    "\n",
    "        print('filtering for businesses in {}...'.format(kwargs['name']))\n",
    "        item = item[item['state'].str.upper().str.replace(' ', '') == kwargs['name']]\n",
    "        item.reset_index(drop=True, inplace=True) #丢掉原来的索引，重新安排从0开始的索引\n",
    "        item_ids = dict(zip(item['item_id'], item.index)) #将zip对象转换成字典， key : value\n",
    "        data = data[data['item_id'].isin(item_ids) & (data['rating'] > 0)].reset_index(drop=True) #获取评论文件中指定的且评分大于零的项目\n",
    "        #isin()接受一个列表，判断该列中元素是否在列表中\n",
    "        user = user[user['user_id'].isin(data['user_id'])].reset_index(drop=True) #将用户文件中评论过的用户挑出来\n",
    "        user_ids = dict(zip(user['user_id'], user.index)) #打包用户文件新的用户ID和索引\n",
    "        data['user_id'] = data['user_id'].apply(user_ids.get) #得到用户id列表\n",
    "        data['item_id'] = data['item_id'].apply(item_ids.get) #得到项目id列表\n",
    "\n",
    "        print('splitting data by year...') #按照年份划分数据集\n",
    "        data['date'] = pd.to_datetime(data['date']) #获取指定的时间和日期，将Str和Unicode转化为时间格式\n",
    "        data['is_test'] = data['date'].dt.year == data['date'].dt.year.max() #标识是否为测试集（最新一年2021为测试集）\n",
    "        data['is_tune'] = data['date'].dt.year == data['date'].dt.year.max() - 1 #标识validation set(2020年)\n",
    "        kwargs['data'] = data\n",
    "        \n",
    "        print('building user graph...') #搭建用户图\n",
    "        row, col = [], []\n",
    "        for i, friends in user['friends'].items(): #i -- 用户，friends -- 该用户的好友列表\n",
    "            for friend in friends.split(', '): #按照逗号将friends分割\n",
    "                if friend in user_ids: #如果朋友在user_id里\n",
    "                    row.append(i) #添加用户i\n",
    "                    col.append(user_ids[friend]) #添加朋友\n",
    "        kwargs['user_graph'] = sp.coo_matrix(([1.0]*len(row), (row, col)), shape=(len(user), len(user))) #按照row,col指示的位置，填入1，表示这两个用户是朋友\n",
    "        \n",
    "        print('processing user features...') #处理用户特征，对用户的评价\n",
    "        compliments = user[[c for c in user.columns if c.startswith('compliment')]] #该用户得到几个赞\n",
    "        #number of hot compliments received by the user\n",
    "        compliments.columns = ['s: '.join(c.split('_')) for c in compliments.columns] #修改列名\n",
    "        kwargs['user_compliments'] = (compliments - compliments.min())/(compliments.max() - compliments.min()) #线性化\n",
    "\n",
    "        votes = user[['cool', 'funny', 'useful']] #投票\n",
    "        votes.columns = ['votes: {}'.format(c) for c in votes.columns]\n",
    "        kwargs['user_votes'] = (votes - votes.min())/(votes.max() - votes.min())\n",
    "\n",
    "        profile = user[['fans']] #该用户粉丝简况\n",
    "        user['yelping_since'] = pd.to_datetime(user['yelping_since']) #加入Yelp时间\n",
    "        profile['yelping_since_year'] = user['yelping_since'].dt.year\n",
    "        profile['yelping_since_month'] = user['yelping_since'].dt.month\n",
    "        profile['yelping_since_day'] = user['yelping_since'].dt.day\n",
    "        user['elite'] = user['elite'].apply(lambda x: [] if x == 'None' else x.split(', ')) \n",
    "        mlb = MultiLabelBinarizer() #构造多标签数据的Label\n",
    "        elite = pd.DataFrame(mlb.fit_transform(user['elite']), columns=['elite_' + c for c in mlb.classes_]) \n",
    "        profile = pd.concat([profile, elite], 1)\n",
    "        kwargs['user_profiles'] = (profile - profile.min())/(profile.max() - profile.min())\n",
    "\n",
    "        kwargs['user_features'] = pd.concat([\n",
    "            kwargs['user_compliments'],\n",
    "            kwargs['user_votes'],\n",
    "            kwargs['user_profiles'],\n",
    "        ], 1)\n",
    "        \n",
    "        \n",
    "        print('processing business categories...') #处理项目种类\n",
    "        categories = item['categories'].apply(lambda x: [] if x is None else x.split(', '))\n",
    "        mlb = MultiLabelBinarizer() #构造多标签数据的Label\n",
    "        categories = pd.DataFrame(mlb.fit_transform(categories), columns=mlb.classes_) #把所有的categories生成一个json文件\n",
    "        df = pd.read_json(path + '/categories.json',lines=True).set_index('alias')\n",
    "        df['parents'] = df['parents'].apply(sorted) #排序\n",
    "        def get_path(node, df=df):\n",
    "            if df.parents[node] == []:\n",
    "                return df.title[node]\n",
    "            return '{}: {}'.format(get_path(df.parents[node][0], df), df.title[node])\n",
    "        df['path'] = df.index.to_series().apply(get_path)\n",
    "        df.set_index('title', inplace=True)\n",
    "        categories.columns = ['categories: {}'.format(df.path.get(c, c)) for c in categories.columns] #项目是否具备某种特征\n",
    "        kwargs['item_categories'] = categories.astype(float)\n",
    "        \n",
    "        print('building business graph...')\n",
    "        kwargs['item_graph'] = kneighbors_graph(kwargs['item_categories'].values, 10) #创建10-Nearest-Neighbor-Graph\n",
    "        \n",
    "        print('processing business attributes...')\n",
    "        attributes = pd.read_csv(path + '/attributes.csv').rename(columns={'business_id': 'item_id'})\n",
    "        attributes = attributes[attributes['item_id'].isin(item_ids)]\n",
    "        attributes['item_id'] = attributes['item_id'].apply(item_ids.get)\n",
    "        attributes = attributes.sort_values('item_id').reset_index(drop=True)\n",
    "        attributes.drop(columns=[\n",
    "            'item_id',\n",
    "            'attributes.RestaurantsPriceRange2.unspecified',\n",
    "            'attributes.NoiseLevel.average',\n",
    "            'attributes.AgesAllowed.allages',\n",
    "            'attributes.Alcohol.unspecified',\n",
    "            'attributes.WiFi.unspecified',\n",
    "            'attributes.RestaurantsAttire.unspecified',\n",
    "            'attributes.Smoking.unspecified',\n",
    "            'attributes.BYOBCorkage.unspecified',\n",
    "        ], inplace=True)\n",
    "        attributes.drop(columns=[c for c in attributes.columns if attributes[c].nunique() == 1], inplace=True)\n",
    "        attributes.columns = [': '.join(c.split('.')) for c in attributes.columns]\n",
    "        kwargs['item_attributes'] = attributes.astype(float)\n",
    "\n",
    "        locations = item[['latitude', 'longitude']]\n",
    "        locations.columns = ['locations: {}'.format(c) for c in locations.columns]\n",
    "        locations = locations.fillna(locations.mean())\n",
    "        kwargs['item_locations'] = (locations - locations.min())/(locations.max() - locations.min())\n",
    "        \n",
    "        print(item['hours'])\n",
    "        hours = item['hours'].apply(lambda x: {} if x is None else literal_eval(str(x)))\n",
    "        hours = pd.json_normalize(hours)\n",
    "        for c in hours.columns:\n",
    "            hours[['hours: {}: Open'.format(c), 'hours: {}: Close'.format(c)]] = hours[c].str.split('-', expand=True)\n",
    "            hours.drop(columns=c, inplace=True)\n",
    "        def to_float(s):\n",
    "            if s is np.nan:\n",
    "                return np.nan\n",
    "            h, m = s.split(':')\n",
    "            return float(h)/24 + float(m)/60/24\n",
    "        hours = hours.applymap(to_float)\n",
    "        kwargs['item_hours'] = hours.fillna(hours.mean())\n",
    "        \n",
    "        print('processing business checkins...')\n",
    "        checkins = pd.read_json(path + '/checkin.json', lines=True)\n",
    "        times = dict(zip(checkins['business_id'], checkins['time']))\n",
    "        checkins = pd.Series([times.get(item_id, {}) for item_id in item['item_id']])\n",
    "        checkins = pd.json_normalize(checkins).fillna(0)\n",
    "        checkins.columns = ['checkins: {}'.format(': '.join(c.split('-'))) for c in checkins.columns]\n",
    "        kwargs['item_checkins'] = (checkins - checkins.min())/(checkins.max() - checkins.min())\n",
    "\n",
    "        kwargs['item_features'] = pd.concat([\n",
    "            kwargs['item_categories'],\n",
    "            kwargs['item_attributes'],\n",
    "            kwargs['item_locations'],\n",
    "            kwargs['item_hours'],\n",
    "            kwargs['item_checkins'],\n",
    "        ], 1)\n",
    "\n",
    "        locations = item[['latitude', 'longitude']]\n",
    "        locations.columns = ['locations: {}'.format(c) for c in locations.columns]\n",
    "        kwargs['item_coordinates'] = locations \n",
    "            \n",
    "        super(Yelp2021, self).__init__(**kwargs)\n",
    "        print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b1b1ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading user data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f28bd42e024d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYelp2021\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'PA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-41e75472ebe2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, state, path, **kwargs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loading user data...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0muser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/user.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'over'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loading business data...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m                 )\n\u001b[1;32m    295\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows)\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    751\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1119\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m             )\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = Yelp2021.__init__('__main__','PA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3f598b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-b3f9ea0eb0ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYelp2021\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'over'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/datasets'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'state'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print('start')\n",
    "    dataset = Yelp2021()\n",
    "    print('over')\n",
    "    dataset.save('data/datasets')\n",
    "    dataset = Dataset.load('data/datasets/Yelp2021')\n",
    "    print(dataset.name)\n",
    "    print(dataset.n_user)\n",
    "    print(dataset.n_item)\n",
    "    print(dataset.n_data)\n",
    "    print(dataset.min)\n",
    "    print(dataset.max)\n",
    "    print(dataset.n_data/float(dataset.n_user*dataset.n_item))\n",
    "    print(len(dataset.train))\n",
    "    print(len(dataset.tune))\n",
    "    print(len(dataset.test))\n",
    "    print(dataset.side_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d368d562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
